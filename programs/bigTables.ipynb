{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"images/dans-small.png\"/>\n",
    "<img align=\"right\" src=\"images/tf-small.png\"/>\n",
    "<img align=\"right\" src=\"images/etcbc.png\"/>\n",
    "\n",
    "# BHSA as a Big Table\n",
    "This notebook exports the [BHSA](etcbc.png) database to an R data frame.\n",
    "The nodes are exported as rows, they correspond to the text objects such as word, phrase, clause, sentence, verse, chapter, book and a few others.\n",
    "\n",
    "The BHSA features become the columns, so each row tells what values the features have for the corresponding object.\n",
    "\n",
    "The edges corresponding to the BHSA features *mother*, *functional_parent*, *distributional_parent* are\n",
    "exported as extra columns. For each row, such a column indicates the target of a corresponding outgoing edge.\n",
    "\n",
    "We also write the data that says which objects are contained in which.\n",
    "To each row we add the following columns:\n",
    "\n",
    "* for each object type, except `word` there is a column with name that object type and containing\n",
    "  the identifier of the containing object of that type of the row object (if any).\n",
    "\n",
    "Extra data such as lexicon (including frequency and rank features), phonetic transcription, and ketiv-qere are also included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compose the big table and save it as a tab delimited file.\n",
    "The result can be processed by R and Pandas,\n",
    "who may converted the table to internal formats\n",
    "for quicker loading.\n",
    "It turns out that for this size of the data Pandas is a bit quicker than R.\n",
    "\n",
    "Also, because we remain in a Python environment, working with Pandas \n",
    "is easier when you want to use configurations ad libraries from the text-fabric sphere.\n",
    "\n",
    "See \n",
    "[bigTablesR](bigTablesR.ipynb)\n",
    "and\n",
    "[bigTablesP](bigTablesP.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, collections\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = '~/github/etcbc'\n",
    "coreModule = 'bhsa'\n",
    "sources = [coreModule, 'phono']\n",
    "version = '2017'\n",
    "tempDir = os.path.expanduser('{}/{}/_temp/{}/r'.format(locations, coreModule, version))\n",
    "tableFile = '{}/{}{}.txt'.format(tempDir, coreModule, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 8.4.13\n",
      "Api reference : https://annotation.github.io/text-fabric/tf/cheatsheet.html\n",
      "\n",
      "118 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "modules = ['{}/tf/{}'.format(s, version) for s in sources]\n",
    "TF = Fabric(locations=locations, modules=modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ALL features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.00s Dataset without structure sections in otext:no structure functions in the T-API\n",
      "  3.57s All features loaded/computed - for details use loadLog()\n",
      "   |     0.00s Feature overview: 111 for nodes; 5 for edges; 2 configs; 8 computed\n",
      "  0.00s loading features ...\n",
      "   |     0.00s Dataset without structure sections in otext:no structure functions in the T-API\n",
      "  7.16s All features loaded/computed - for details use loadLog()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Computed',\n",
       "  'computed-data',\n",
       "  ('C Computed', 'Call AllComputeds', 'Cs ComputedString')),\n",
       " ('Features', 'edge-features', ('E Edge', 'Eall AllEdges', 'Es EdgeString')),\n",
       " ('Fabric', 'loading', ('TF',)),\n",
       " ('Locality', 'locality', ('L Locality',)),\n",
       " ('Nodes', 'navigating-nodes', ('N Nodes',)),\n",
       " ('Features',\n",
       "  'node-features',\n",
       "  ('F Feature', 'Fall AllFeatures', 'Fs FeatureString')),\n",
       " ('Search', 'search', ('S Search',)),\n",
       " ('Text', 'text', ('T Text',))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = TF.load('')\n",
    "allFeatures = TF.explore(silent=False, show=True)\n",
    "loadableFeatures = allFeatures['nodes'] + allFeatures['edges']\n",
    "api = TF.load(loadableFeatures)\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    27s  100000 nodes written\n",
      "    41s  200000 nodes written\n",
      "    55s  300000 nodes written\n",
      " 1m 09s  400000 nodes written\n",
      " 1m 23s  500000 nodes written\n",
      " 1m 37s  600000 nodes written\n",
      " 1m 52s  700000 nodes written\n",
      " 2m 06s  800000 nodes written\n",
      " 2m 20s  900000 nodes written\n",
      " 2m 34s 1000000 nodes written\n",
      " 2m 48s 1100000 nodes written\n",
      " 3m 02s 1200000 nodes written\n",
      " 3m 16s 1300000 nodes written\n",
      " 3m 30s 1400000 nodes written\n",
      " 3m 36s 1446635 nodes written and done\n"
     ]
    }
   ],
   "source": [
    "## info(\"Writing R feature data\")\n",
    "\n",
    "if not os.path.exists(tempDir):\n",
    "    os.makedirs(tempDir)\n",
    "\n",
    "hr = open(tableFile, 'w')\n",
    "\n",
    "skipFeatures = '''\n",
    "    otype\n",
    "    oslots\n",
    "'''.strip().split()\n",
    "for f in (Fall() + Eall()):\n",
    "    if '@' in f: skipFeatures.append(f)\n",
    "\n",
    "levelFeatures = '''\n",
    "    lex subphrase phrase_atom phrase clause_atom clause sentence_atom sentence\n",
    "    half_verse verse chapter book\n",
    "'''.strip().split()\n",
    "inLevelFeatures = ['in.'+x for x in levelFeatures]\n",
    "\n",
    "allNodeFeatures = sorted(set(Fall()) - set(skipFeatures))\n",
    "allEdgeFeatures = sorted(set(Eall()) - set(skipFeatures))\n",
    "\n",
    "hr.write('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
    "    'n',\n",
    "    'otype',\n",
    "    '\\t'.join(inLevelFeatures),\n",
    "    '\\t'.join(allEdgeFeatures),\n",
    "    '\\t'.join(allNodeFeatures),\n",
    "))\n",
    "chunkSize = 100000\n",
    "i = 0\n",
    "s = 0\n",
    "NA = ['']\n",
    "NAe = [['']]\n",
    "for n in N.walk():\n",
    "    levelValues = [(L.u(n, otype=level) or NA)[0] for level in levelFeatures]\n",
    "    edgeValues = [str((Es(f).f(n) or NA)[0]) for f in allEdgeFeatures]\n",
    "    nodeValues = [(str(Fs(f).v(n) or '')) for f in allNodeFeatures]\n",
    "    hr.write('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
    "        n,\n",
    "        F.otype.v(n),\n",
    "        ('\\t'.join(str(x) for x in levelValues)),\n",
    "        ('\\t'.join(edgeValues)),\n",
    "        ('\\t'.join(nodeValues)).replace('\\n',''),\n",
    "    ))\n",
    "    i += 1\n",
    "    s += 1\n",
    "    if s == chunkSize:\n",
    "        s = 0\n",
    "        TF.info('{:>7} nodes written'.format(i))\n",
    "hr.close()\n",
    "TF.info('{:>7} nodes written and done'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 720904\n",
      "-rw-r--r--  1 dirk  staff   340M Apr  7 21:25 bhsa2017.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {tempDir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tabular export is ready now, but it is a bit large.\n",
    "We can get a much leaner file by using R to load this file and save it in .rds format.\n",
    "\n",
    "We do that in a separate notebook, not running Python, but R: bigTablesR in this same directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
