{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"images/dans-small.png\"/>\n",
    "<img align=\"right\" src=\"images/tf-small.png\"/>\n",
    "<img align=\"right\" src=\"images/etcbc.png\"/>\n",
    "\n",
    "\n",
    "# Lexicon\n",
    "\n",
    "This notebook can read lexicon info in files issued by the ETCBC and transform them\n",
    "into new features.\n",
    "There will be new features at the word level and a new level will be made: lexeme.\n",
    "\n",
    "Most lexical features do not go to the word nodes but to the lexeme nodes.\n",
    "\n",
    "**NB** This conversion will not work for versions `4` and `4b`.\n",
    "\n",
    "## Discussion\n",
    "There are several issues that we deal with here.\n",
    "\n",
    "Language: are an Aramaic lexeme and a Hebrew lexeme with the same value identical?\n",
    "In short: no.\n",
    "\n",
    "The lexicon is a piece of data not conceptually contained in the text.\n",
    "So where do we leave that data?\n",
    "\n",
    "The lexicon contains information about lexemes. Some of that information is also present\n",
    "on individual occurrences.\n",
    "The question arises, should a lexical feature have consistent values across its occurrences.\n",
    "\n",
    "And of course: does the lexicon *match* the text?\n",
    "Do all lexemes in the text have a lexical entry, and does every lexical entry have actual\n",
    "occurrences in the text?\n",
    "\n",
    "### Lexeme language\n",
    "Lexemes do not cross languages, so the set of Aramaic and Hebrew lexemes are disjoint.\n",
    "Whenever we index lexemes, we have to specify it as a pair of its language and lex values.\n",
    "\n",
    "### Lexeme node type\n",
    "The answer where to leave the lexical information in a text-fabric data set is surprisingly simple:\n",
    "on nodes of a new type `lex`.\n",
    "Nodes of type lex will be connected via the `oslots` feature to all its occurrences, so lexemes *contain* there\n",
    "occurrences.\n",
    "All features encountered in the lexicon, we will store on these `lex` nodes.\n",
    "\n",
    "However, experience has taught us that it is very convenient to duplicate feature values of lexeme nodes to all\n",
    "corresponding word nodes.\n",
    "This is what we will do.\n",
    "\n",
    "### Lexical consistency\n",
    "It is quite possible that some occurrences have got a different value for a lexical feature than its lexeme.\n",
    "A trivial case are adjectives, whose lexical gender is `NA`, but whose occurrences usually have a distinct gender.\n",
    "\n",
    "Other features really should be fully consistent, for example the *vocalized lexeme*.\n",
    "We encounter this feature in the text (`g_voc_lex` and also its Hebrew version `g_voc_lex_utf8`),\n",
    "and in the lexicon it is present as feature `vc`.\n",
    "In this case we observe a deficiency in the lexicon: `vc` is often absent.\n",
    "Apart from that, the textual features `g_voc_lex` and `g_voc_lex_utf8` are fully consistent, so I take their values\n",
    "and put them in the lexicon, and I remove the `g_voc_lex` and `g_voc_lex_utf8` from the dataset.\n",
    "\n",
    "## Match between lexicon and text\n",
    "We perform quite a number of checks.\n",
    "The match should be perfect.\n",
    "If not, then quite possible the MQL core data has been exported at an other time the the lexical data.\n",
    "\n",
    "## Various issues\n",
    "1. `lex` contains the lexeme (in transcription) with disambiguation marks (`[/=`) appended.\n",
    "   For text transformations we prefer the bare lexeme, and we store that in a new feature `lex0`\n",
    "1. `lex_utf` has frills at the end of many values.\n",
    "   They occur where the final consonant as an alternative form. See analysis below.\n",
    "1. `language` has values `Hebrew` and `Aramaic`. We prefer ISO language codes: `hbo` and `arc` instead.\n",
    "   By adding `language` for lexeme nodes we already have switched to ISO codes. Here we do the rest.\n",
    "1. the feature lex_utf8 occurs only on word nodes, but is consistent across lexemes. We add it to lexeme nodes\n",
    "   as well, together with `lex0`.\n",
    "\n",
    "We are going to deal with these issues [later](#Deal-with-various-issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import yaml\n",
    "from tf.fabric import Fabric\n",
    "from tf.core.helpers import formatMeta\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "See [operation](https://github.com/ETCBC/pipeline/blob/master/README.md#operation)\n",
    "for how to run this script in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SCRIPT\" not in locals():\n",
    "    SCRIPT = False\n",
    "    FORCE = True\n",
    "    CORE_NAME = \"bhsa\"\n",
    "    EXTRA_OVERLAP = \"\"\n",
    "    #    EXTRA_OVERLAP='gloss nametype'\n",
    "    DO_VOCALIZED_LEXEME = True\n",
    "    #    DO_VOCALIZED_LEXEME=False\n",
    "    LEX_FORMATS = \"@fmt:lex-trans-plain={lex0} \"\n",
    "    #    LEX_FORMATS='@fmt:lex-trans-plain={lex} '\n",
    "\n",
    "    VERSION = \"2021\"\n",
    "\n",
    "\n",
    "def stop(good=False):\n",
    "    if SCRIPT:\n",
    "        sys.exit(0 if good else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of lex_utf8\n",
    "\n",
    "Let us focus on a few cases.\n",
    "\n",
    "We translate the UTF sequences found in the MQL source into real Unicode characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "al        :\n",
      "        MQL original   = \\xd7\\xa2\\xd7\\x9c\n",
      "        Unicode        = על\n",
      "        Last char id   =  5dc HEBREW LETTER LAMED\n",
      "        Last char uni  = ל\n",
      "    \n",
      "b         :\n",
      "        MQL original   = \\xd7\\x91\n",
      "        Unicode        = ב\n",
      "        Last char id   =  5d1 HEBREW LETTER BET\n",
      "        Last char uni  = ב\n",
      "    \n",
      "lhjm      :\n",
      "        MQL original   = \\xd7\\x90\\xd7\\x9c\\xd7\\x94\\xd7\\x99\\xd7\\x9d\\xd6\\x9c\n",
      "        Unicode        = אלהים֜\n",
      "        Last char id   =  59c HEBREW ACCENT GERESH\n",
      "        Last char uni  = ֜\n",
      "    \n",
      "rcjt      :\n",
      "        MQL original   = \\xd7\\xa8\\xd7\\x90\\xd7\\xa9\\xd7\\x81\\xd7\\x99\\xd7\\xaa\\xd6\\x9c\n",
      "        Unicode        = ראשׁית֜\n",
      "        Last char id   =  59c HEBREW ACCENT GERESH\n",
      "        Last char uni  = ֜\n",
      "    \n",
      "rcjt_nme  :\n",
      "        MQL original   = \\xd6\\x9c\n",
      "        Unicode        = ֜\n",
      "        Last char id   =  59c HEBREW ACCENT GERESH\n",
      "        Last char uni  = ֜\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if not SCRIPT:\n",
    "    import unicodedata\n",
    "\n",
    "    uniscan = re.compile(r\"(?:\\\\x..)+\")\n",
    "\n",
    "    def makeuni(match):\n",
    "        \"\"\"Make proper unicode of a text that contains byte escape codes such as backslash xb6\"\"\"\n",
    "        byts = eval('\"' + match.group(0) + '\"')\n",
    "        return byts.encode(\"latin1\").decode(\"utf-8\")\n",
    "\n",
    "    def uni(line):\n",
    "        return uniscan.sub(makeuni, line)\n",
    "\n",
    "    cases = dict(\n",
    "        b=r\"\\xd7\\x91\",\n",
    "        rcjt=r\"\\xd7\\xa8\\xd7\\x90\\xd7\\xa9\\xd7\\x81\\xd7\\x99\\xd7\\xaa\\xd6\\x9c\",\n",
    "        rcjt_nme=r\"\\xd6\\x9c\",\n",
    "        lhjm=r\"\\xd7\\x90\\xd7\\x9c\\xd7\\x94\\xd7\\x99\\xd7\\x9d\\xd6\\x9c\",\n",
    "        al=r\"\\xd7\\xa2\\xd7\\x9c\",\n",
    "    )\n",
    "\n",
    "    for (case, utf8) in sorted(cases.items()):\n",
    "        uword = uni(utf8)\n",
    "        uLast = uword[-1]\n",
    "        uCode = ord(uLast)\n",
    "        uName = unicodedata.name(uLast)\n",
    "        print(\n",
    "            \"\"\"{:<10}:\n",
    "        MQL original   = {}\n",
    "        Unicode        = {}\n",
    "        Last char id   = {:>4x} {}\n",
    "        Last char uni  = {}\n",
    "    \"\"\".format(\n",
    "                case,\n",
    "                utf8,\n",
    "                uword,\n",
    "                uCode,\n",
    "                uName,\n",
    "                uLast,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the context: source file and target directories\n",
    "\n",
    "The conversion is executed in an environment of directories, so that sources, temp files and\n",
    "results are in convenient places and do not have to be shifted around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repoBase = os.path.expanduser(\"~/github/etcbc\")\n",
    "thisRepo = \"{}/{}\".format(repoBase, CORE_NAME)\n",
    "\n",
    "thisSource = \"{}/source/{}\".format(thisRepo, VERSION)\n",
    "\n",
    "thisTemp = \"{}/_temp/{}\".format(thisRepo, VERSION)\n",
    "thisTempSource = \"{}/source\".format(thisTemp)\n",
    "thisTempTf = \"{}/tf\".format(thisTemp)\n",
    "\n",
    "thisTf = \"{}/tf/{}\".format(thisRepo, VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeature = \"lex0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Check whether this conversion is needed in the first place.\n",
    "Only when run as a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCRIPT:\n",
    "    (good, work) = utils.mustRun(\n",
    "        None, \"{}/.tf/{}.tfx\".format(thisTf, testFeature), force=FORCE\n",
    "    )\n",
    "    if not good:\n",
    "        stop(good=False)\n",
    "    if not work:\n",
    "        stop(good=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Settings\n",
    "\n",
    "* a piece of metadata that will go into these features; the time will be added automatically\n",
    "* new text formats for the `otext` feature of TF, based on lexical features.\n",
    "  We select the version specific otext material,\n",
    "  falling back on a default if nothing appropriate has been specified in oText.\n",
    "\n",
    "We do not do this for the older versions `4` and `4b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       0.00s New text formats\n",
      "|       0.00s fmt:lex-trans-plain            = \"{lex0} \"\n"
     ]
    }
   ],
   "source": [
    "genericMetaPath = f\"{thisRepo}/yaml/generic.yaml\"\n",
    "coreMetaPath = f\"{thisRepo}/yaml/core.yaml\"\n",
    "lexiconMetaPath = f\"{thisRepo}/yaml/lexicon.yaml\"\n",
    "\n",
    "with open(genericMetaPath) as fh:\n",
    "    genericMeta = yaml.load(fh, Loader=yaml.FullLoader)\n",
    "    genericMeta[\"version\"] = VERSION\n",
    "with open(coreMetaPath) as fh:\n",
    "    coreMeta = formatMeta(yaml.load(fh, Loader=yaml.FullLoader))\n",
    "with open(lexiconMetaPath) as fh:\n",
    "    lexiconMeta = formatMeta(yaml.load(fh, Loader=yaml.FullLoader))\n",
    "\n",
    "metaData = {\"\": genericMeta}\n",
    "\n",
    "lexType = \"lex\"\n",
    "\n",
    "if LEX_FORMATS == \"\":\n",
    "    utils.caption(0, \"No additional text formats provided\")\n",
    "    otextInfo = {}\n",
    "else:\n",
    "    utils.caption(0, \"New text formats\")\n",
    "    otextInfo = dict(\n",
    "        line[1:].split(\"=\", 1) for line in LEX_FORMATS.strip(\"\\n\").split(\"\\n\")\n",
    "    )\n",
    "    for x in sorted(otextInfo.items()):\n",
    "        utils.caption(0, '{:<30} = \"{}\"'.format(*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon preparation\n",
    "We add lexical data.\n",
    "The lexical data will not be added as features of words, but as features of lexemes.\n",
    "The lexemes will be added as fresh nodes, of a new type `lex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".       4.94s Load the existing TF dataset                                                   .\n",
      "..............................................................................................\n",
      "This is Text-Fabric 9.1.6\n",
      "Api reference : https://annotation.github.io/text-fabric/tf/cheatsheet.html\n",
      "\n",
      "101 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.00s Dataset without structure sections in otext:no structure functions in the T-API\n",
      "    11s All features loaded/computed - for details use TF.isLoaded()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Computed',\n",
       "  'computed-data',\n",
       "  ('C Computed', 'Call AllComputeds', 'Cs ComputedString')),\n",
       " ('Features', 'edge-features', ('E Edge', 'Eall AllEdges', 'Es EdgeString')),\n",
       " ('Fabric', 'loading', ('TF',)),\n",
       " ('Locality', 'locality', ('L Locality',)),\n",
       " ('Nodes', 'navigating-nodes', ('N Nodes',)),\n",
       " ('Features',\n",
       "  'node-features',\n",
       "  ('F Feature', 'Fall AllFeatures', 'Fs FeatureString')),\n",
       " ('Search', 'search', ('S Search',)),\n",
       " ('Text', 'text', ('T Text',))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.caption(4, \"Load the existing TF dataset\")\n",
    "TF = Fabric(locations=thisTf, modules=[\"\"])\n",
    "vocLex = \" g_voc_lex g_voc_lex_utf8 \" if DO_VOCALIZED_LEXEME else \"\"\n",
    "api = TF.load(\n",
    "    \"lex lex_utf8 language sp ls gn ps nu st oslots {} {}\".format(vocLex, EXTRA_OVERLAP)\n",
    ")\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text pass\n",
    "We map the values in the language feature to standardized ISO values: `arc` and `hbo`.\n",
    "We run over all word occurrences, grab the language and lexeme identifier, and create for each\n",
    "unique pair a new lexeme node.\n",
    "\n",
    "We remember the mapping between nodes and lexemes.\n",
    "\n",
    "We check whether the word features `lex_utf8` and `g_lex_utf` are consistent between occurrences\n",
    "of the same lexeme.\n",
    "\n",
    "This stage does not yet involve the lexical files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         18s Collect lexemes from the text                                                  .\n",
      "..............................................................................................\n",
      "|         21s Check consistency of lex_utf8, g_lex_utf8\n",
      "|         21s lex_utf8 is consistent over lexeme occurrences\n",
      "|         21s 4122 inconsistencies in g_lex_utf8\n",
      "|         21s added 9230 lexemes\n",
      "|         21s maxNode is now 1446831\n",
      "|         21s language arc has   708 lexemes in the text\n",
      "|         21s language hbo has  8522 lexemes in the text\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, \"Collect lexemes from the text\")\n",
    "\n",
    "langMap = {\n",
    "    \"hbo\": \"hbo\",\n",
    "    \"Hebrew\": \"hbo\",\n",
    "    \"Aramaic\": \"arc\",\n",
    "    \"arc\": \"arc\",\n",
    "}\n",
    "langIMap = {\n",
    "    \"hbo\": \"Hebrew\",\n",
    "    \"Hebrew\": \"Hebrew\",\n",
    "    \"Aramaic\": \"Aramaic\",\n",
    "    \"arc\": \"Aramaic\",\n",
    "}\n",
    "\n",
    "doValueCompare = {\"sp\", \"ls\", \"gn\", \"ps\", \"nu\", \"st\"}\n",
    "doFeatureCheck = {\"lex_utf8\", \"g_lex_utf8\"}\n",
    "\n",
    "lexText = {}\n",
    "\n",
    "maxNode = F.otype.maxNode\n",
    "maxSlot = F.otype.maxSlot\n",
    "slotType = F.otype.slotType\n",
    "\n",
    "lexNode = maxNode\n",
    "lexOccs = {}\n",
    "nodeFromLex = {}\n",
    "lexFromNode = {}\n",
    "otypeData = {}\n",
    "oslotsData = {}\n",
    "\n",
    "for n in F.otype.s(\"word\"):\n",
    "    lex = F.lex.v(n)\n",
    "    lan = langMap[F.language.v(n)]\n",
    "    lexId = (lan, lex)\n",
    "    lexOccs.setdefault(lexId, []).append(n)\n",
    "\n",
    "    for ft in doValueCompare:\n",
    "        val = Fs(ft).v(n)\n",
    "        lexText.setdefault(lan, {}).setdefault(lex, {}).setdefault(ft, set()).add(val)\n",
    "\n",
    "    if lexId not in nodeFromLex:\n",
    "        lexNode += 1\n",
    "        nodeFromLex[lexId] = lexNode\n",
    "        lexFromNode[lexNode] = lexId\n",
    "        \n",
    "utils.caption(0, f\"Check consistency of {', '.join(doFeatureCheck)}\")\n",
    "inconsistent = {ft: 0 for ft in doFeatureCheck}\n",
    "\n",
    "for (lexId, ws) in lexOccs.items():\n",
    "    for ft in doFeatureCheck:\n",
    "        values = {Fs(ft).v(w) for w in ws}\n",
    "        if len(values) != 1:\n",
    "            inconsistent[ft] += 1\n",
    "for ft in doFeatureCheck:\n",
    "    nInc = inconsistent[ft]\n",
    "    utils.caption(0, f\"{nInc} inconsistencies in {ft}\" if nInc else f\"{ft} is consistent over lexeme occurrences\")\n",
    "\n",
    "for n in range(maxNode + 1, lexNode + 1):\n",
    "    otypeData[n] = \"lex\"\n",
    "    oslotsData[n] = lexOccs[lexFromNode[n]]\n",
    "\n",
    "utils.caption(0, \"added {} lexemes\".format(len(nodeFromLex)))\n",
    "utils.caption(0, \"maxNode is now {}\".format(lexNode))\n",
    "\n",
    "for lan in sorted(lexText):\n",
    "    utils.caption(\n",
    "        0, \"language {} has {:>5} lexemes in the text\".format(lan, len(lexText[lan]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon pass\n",
    "Here we are going to read the lexicons, one for Aramaic, and one for Hebrew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         24s Collect lexeme info from the lexicon                                           .\n",
      "..............................................................................................\n",
      "|         24s Reading lexicon ...\n",
      "|         24s Lexicon arc has   708 entries\n",
      "|         24s Lexicon hbo has  8522 entries\n",
      "|         24s Done\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, \"Collect lexeme info from the lexicon\")\n",
    "\n",
    "langs = set(langMap.values())\n",
    "lexFile = dict((lan, \"{}/lexicon_{}.txt\".format(thisSource, lan)) for lan in langs)\n",
    "\n",
    "\n",
    "def readLex(lan):\n",
    "    lexInfile = open(lexFile[lan], encoding=\"utf-8\")\n",
    "    errors = []\n",
    "\n",
    "    lexItems = {}\n",
    "    ln = 0\n",
    "    for line in lexInfile:\n",
    "        ln += 1\n",
    "        line = line.rstrip()\n",
    "        line = line.split(\"#\")[0]\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        (entry, featurestr) = line.split(sep=None, maxsplit=1)\n",
    "        entry = entry.strip('\"')\n",
    "        if entry in lexItems:\n",
    "            errors.append(\"duplicate lexical entry {} in line {}.\\n\".format(entry, ln))\n",
    "            continue\n",
    "        featurestr = featurestr.strip(\":\")\n",
    "        featurestr = featurestr.replace(\"\\\\:\", chr(254))\n",
    "        featurelst = featurestr.split(\":\")\n",
    "        features = {}\n",
    "        for feature in featurelst:\n",
    "            comps = feature.split(\"=\", maxsplit=1)\n",
    "            if len(comps) == 1:\n",
    "                if feature.strip().isnumeric():\n",
    "                    comps = (\"_n\", feature.strip())\n",
    "                else:\n",
    "                    errors.append(\n",
    "                        \"feature without value for lexical entry {} in line {}: {}\\n\".format(\n",
    "                            entry,\n",
    "                            ln,\n",
    "                            feature,\n",
    "                        )\n",
    "                    )\n",
    "                    continue\n",
    "            (key, value) = comps\n",
    "            value = value.replace(chr(254), \":\")\n",
    "            if key in features:\n",
    "                errors.append(\n",
    "                    \"duplicate feature for lexical entry {} in line {}: {}={}\\n\".format(\n",
    "                        entry,\n",
    "                        ln,\n",
    "                        key,\n",
    "                        value,\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "            features[key] = value.replace(\"\\\\\", \"/\")\n",
    "        if \"sp\" in features and features[\"sp\"] == \"verb\":\n",
    "            if \"gl\" in features:\n",
    "                gloss = features[\"gl\"]\n",
    "                if gloss.startswith(\"to \"):\n",
    "                    features[\"gl\"] = gloss[3:]\n",
    "        lexItems[entry] = features\n",
    "\n",
    "    lexInfile.close()\n",
    "    nErrors = len(errors)\n",
    "    if len(errors):\n",
    "        utils.caption(\n",
    "            0,\n",
    "            \"Lexicon [{}]: {} error{}\".format(\n",
    "                lan, nErrors, \"\" if nErrors == 1 else \"s\"\n",
    "            ),\n",
    "        )\n",
    "        for error in errors:\n",
    "            utils.caption(1, error)\n",
    "    return lexItems\n",
    "\n",
    "\n",
    "utils.caption(0, \"Reading lexicon ...\")\n",
    "lexEntries = dict((lan, readLex(lan)) for lan in sorted(langs))\n",
    "for lan in sorted(lexEntries):\n",
    "    utils.caption(0, \"Lexicon {} has {:>5} entries\".format(lan, len(lexEntries[lan])))\n",
    "utils.caption(0, \"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "## Matching of text and lexicon\n",
    "\n",
    "Let us now check whether all lexemes in the text occur in the lexicon and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         26s Test - Match between text and lexicon                                          .\n",
      "..............................................................................................\n",
      "|         26s 708 arc lexemes\n",
      "|         26s 8522 hbo lexemes\n",
      "|         26s Equal lex values in hbo and arc in the BHSA   text contains 461 lexemes\n",
      "|         26s Equal lex values in hbo and arc in the lexicon     contains 461 lexemes\n",
      "|         26s Common values in the lexicon but not in the text: 0x: set()\n",
      "|         26s Common values in the text but not in the lexicon: 0x: set()\n",
      "|         26s arc: lexemes in text but not in lexicon: 0x\n",
      "|         26s arc: lexemes in lexicon but not in text: 0x\n",
      "|         26s hbo: lexemes in text but not in lexicon: 0x\n",
      "|         26s hbo: lexemes in lexicon but not in text: 0x\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, \"Test - Match between text and lexicon\")\n",
    "\n",
    "arcLex = set(lexEntries[\"arc\"])\n",
    "hboLex = set(lexEntries[\"hbo\"])\n",
    "\n",
    "utils.caption(0, \"{} arc lexemes\".format(len(arcLex)))\n",
    "utils.caption(0, \"{} hbo lexemes\".format(len(hboLex)))\n",
    "\n",
    "arcText = set(lexText[\"arc\"])\n",
    "hboText = set(lexText[\"hbo\"])\n",
    "\n",
    "hboAndArcText = arcText & hboText\n",
    "hboAndArcLex = arcLex & hboLex\n",
    "\n",
    "lexMinText = hboAndArcLex - hboAndArcText\n",
    "textMinLex = hboAndArcText - hboAndArcLex\n",
    "\n",
    "utils.caption(\n",
    "    0,\n",
    "    \"Equal lex values in hbo and arc in the BHSA   text contains {} lexemes\".format(\n",
    "        len(hboAndArcText)\n",
    "    ),\n",
    ")\n",
    "utils.caption(\n",
    "    0,\n",
    "    \"Equal lex values in hbo and arc in the lexicon     contains {} lexemes\".format(\n",
    "        len(hboAndArcLex)\n",
    "    ),\n",
    ")\n",
    "utils.caption(\n",
    "    0,\n",
    "    \"Common values in the lexicon but not in the text: {}x: {}\".format(\n",
    "        len(lexMinText), lexMinText\n",
    "    ),\n",
    ")\n",
    "utils.caption(\n",
    "    0,\n",
    "    \"Common values in the text but not in the lexicon: {}x: {}\".format(\n",
    "        len(textMinLex), textMinLex\n",
    "    ),\n",
    ")\n",
    "\n",
    "arcTextMinLex = arcText - arcLex\n",
    "arcLexMinText = arcLex - arcText\n",
    "\n",
    "hboTextMinLex = hboText - hboLex\n",
    "hboLexMinText = hboLex - hboText\n",
    "\n",
    "for (myset, mymsg) in (\n",
    "    (arcTextMinLex, \"arc: lexemes in text but not in lexicon\"),\n",
    "    (arcLexMinText, \"arc: lexemes in lexicon but not in text\"),\n",
    "    (hboTextMinLex, \"hbo: lexemes in text but not in lexicon\"),\n",
    "    (hboLexMinText, \"hbo: lexemes in lexicon but not in text\"),\n",
    "):\n",
    "    utils.caption(\n",
    "        0,\n",
    "        \"{}: {}x{}\".format(\n",
    "            mymsg,\n",
    "            len(myset),\n",
    "            \"\" if not myset else \"\\n\\t{}\".format(\", \".join(sorted(myset))),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency of vocalized lexeme\n",
    "\n",
    "The lexicon file provides an attribute `vc` for each lexeme, which is the vocalized lexeme.\n",
    "The BHSA core data also has features `g_voc_lex` and `g_voc_lex_utf8` for each occurrence.\n",
    "\n",
    "We investigate whether the latter features are *consistent*, i.e. a property of the lexeme and lexeme only.\n",
    "If they are somehow dependent on the word occurrence, they are not consistent.\n",
    "\n",
    "When they are consistent, we can omit them on the occurrences and use them on the lexemes.\n",
    "We'll also check whether the `vc` property found in the lexicon coincides with the `g_voc_lex` on the occurrences.\n",
    "\n",
    "Supposing it is all consistent, we will call the new lexeme features `voc_lex` and `voc_lex_utf8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         29s Test - Consistency of vocalized lexeme                                         .\n",
      "..............................................................................................\n",
      "|         30s lexemes with missing vc property: 342x\n",
      "|         30s \thbo-<BC[ supplied from occurrence: <BC\n",
      "|         30s \thbo-<BR=[ supplied from occurrence: <BR\n",
      "|         30s \thbo-<BT[ supplied from occurrence: <BT\n",
      "|         30s \thbo-<BV=[ supplied from occurrence: <BV\n",
      "|         30s \thbo-<DN[ supplied from occurrence: <DN\n",
      "|         30s \thbo-<DP[ supplied from occurrence: <DP\n",
      "|         30s \thbo-<DR==[ supplied from occurrence: <DR\n",
      "|         30s \thbo-<DR=[ supplied from occurrence: <DR\n",
      "|         30s \thbo-<DR[ supplied from occurrence: <DR\n",
      "|         30s \thbo-<FQ[ supplied from occurrence: <FQ\n",
      "|         30s \thbo-<FR[ supplied from occurrence: <FR\n",
      "|         30s \thbo-<GB[ supplied from occurrence: <GB\n",
      "|         30s \thbo-<GM[ supplied from occurrence: <GM\n",
      "|         30s \thbo-<GN[ supplied from occurrence: <GN\n",
      "|         30s \thbo-<KS[ supplied from occurrence: <KS\n",
      "|         30s \thbo-<L<[ supplied from occurrence: <L<\n",
      "|         30s \thbo-<LS[ supplied from occurrence: <LS\n",
      "|         30s \thbo-<MR[ supplied from occurrence: <MR\n",
      "|         30s \thbo-<MS[ supplied from occurrence: <MS\n",
      "|         30s \thbo-<ND[ supplied from occurrence: <ND\n",
      "|         30s Have all occurrences of a lexeme the same voc_lex value?\n",
      "|         30s \tFully consistent\n",
      "|         30s Have all occurrences of a lexeme the same voc_lex_utf8 value?\n",
      "|         30s \tFully consistent\n",
      "|         30s Are the voc_lex values of the lexeme consistent with the vc value of the lexeme?\n",
      "|         30s \t1426 inconsistent cases\n",
      "|         30s \t\thbo-BR>[: B.@R@>, BR>\n",
      "|         30s \t\thbo-HJH[: H@J@H, HJH\n",
      "|         30s \t\thbo-RXP[: R@XAP, RXP\n",
      "|         30s \t\thbo->MR[: >@MAR, >MR\n",
      "|         30s \t\thbo-R>H[: R>H, R@>@H\n",
      "|         30s \t\thbo-VWB[: VOWB, VWB\n",
      "|         30s \t\thbo-BDL[: B.@DAL, BDL\n",
      "|         30s \t\thbo-QR>[: Q@R@>, QR>\n",
      "|         30s \t\thbo-<FH[: <@F@H, <FH\n",
      "|         30s \t\thbo-QWH=[: Q@W@H, QWH\n",
      "|         30s \t\t...and 1416 more.\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, \"Test - Consistency of vocalized lexeme\")\n",
    "\n",
    "if not DO_VOCALIZED_LEXEME:\n",
    "    utils.caption(0, \"\\tSKIPPED in version {}\".format(VERSION))\n",
    "else:\n",
    "    vocFeatures = dict(voc_lex={}, voc_lex_utf8={})\n",
    "\n",
    "    exceptions = dict(incons=dict((f, {}) for f in vocFeatures), deviating=dict())\n",
    "\n",
    "    missing = {}\n",
    "\n",
    "    def showExceptions(cases):\n",
    "        nCases = len(cases)\n",
    "        if nCases == 0:\n",
    "            utils.caption(0, \"\\tFully consistent\")\n",
    "        else:\n",
    "            utils.caption(0, \"\\t{} inconsistent cases\".format(nCases))\n",
    "            limit = 10\n",
    "            for (i, (lan, lex)) in enumerate(cases):\n",
    "                if i == limit:\n",
    "                    utils.caption(0, \"\\t\\t...and {} more.\".format(nCases - limit))\n",
    "                    break\n",
    "                utils.caption(\n",
    "                    0,\n",
    "                    \"\\t\\t{}-{}: {}\".format(\n",
    "                        lan, lex, \", \".join(sorted(cases[(lan, lex)]))\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "    for w in F.otype.s(\"word\"):\n",
    "        lan = langMap[F.language.v(w)]\n",
    "        lex = F.lex.v(w)\n",
    "        for (f, values) in vocFeatures.items():\n",
    "            current = values.get((lan, lex), None)\n",
    "            new = Fs(\"g_{}\".format(f)).v(w)\n",
    "            if current is None:\n",
    "                values[(lan, lex)] = new\n",
    "                if f == \"voc_lex\":\n",
    "                    lexical = lexEntries[lan][lex].get(\"vc\", None)\n",
    "                    if lexical is None:\n",
    "                        missing[(lan, lex)] = new\n",
    "                    else:\n",
    "                        if lexical != new:\n",
    "                            exceptions[\"deviating\"].setdefault(\n",
    "                                (lan, lex), {lexical}\n",
    "                            ).add(new)\n",
    "            else:\n",
    "                if current != new:\n",
    "                    exceptions[\"incons\"][f].setdefault((lan, lex), {current}).add(new)\n",
    "\n",
    "    nMissing = len(missing)\n",
    "\n",
    "    utils.caption(0, \"lexemes with missing vc property: {}x\".format(nMissing))\n",
    "    for (lan, lex) in sorted(missing)[0:20]:\n",
    "        utils.caption(\n",
    "            0,\n",
    "            \"\\t{}-{} supplied from occurrence: {}\".format(\n",
    "                lan, lex, vocFeatures[\"voc_lex\"][(lan, lex)]\n",
    "            ),\n",
    "        )\n",
    "    for f in vocFeatures:\n",
    "        utils.caption(\n",
    "            0, \"Have all occurrences of a lexeme the same {} value?\".format(f)\n",
    "        )\n",
    "        showExceptions(exceptions[\"incons\"][f])\n",
    "    utils.caption(\n",
    "        0,\n",
    "        \"Are the voc_lex values of the lexeme consistent with the vc value of the lexeme?\",\n",
    "    )\n",
    "    showExceptions(exceptions[\"deviating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare TF features\n",
    "\n",
    "We now collect the lexical information into the features for nodes of type `lex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         38s Prepare TF lexical features                                                    .\n",
      "..............................................................................................\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, \"Prepare TF lexical features\")\n",
    "\n",
    "nodeFeatures = {}\n",
    "\n",
    "lexFields = (\n",
    "    (\"rt\", \"root\"),\n",
    "    (\"sp\", \"sp\"),\n",
    "    (\"sm\", \"nametype\"),\n",
    "    (\"ls\", \"ls\"),\n",
    "    (\"gl\", \"gloss\"),\n",
    ")\n",
    "\n",
    "overlapFeatures = {\"lex\", \"language\", \"sp\", \"ls\"} | set(EXTRA_OVERLAP.strip().split())\n",
    "# these are features that occur both on word- and lex- otypes\n",
    "\n",
    "extendFeatures = {\"root\", \"nametype\", \"gloss\"}\n",
    "# these are features coming from the lexicon and not yet present on words\n",
    "# they will be made present on words\n",
    "\n",
    "for f in overlapFeatures:\n",
    "    nodeFeatures[f] = dict((n, Fs(f).v(n)) for n in N.walk() if Fs(f).v(n) is not None)\n",
    "\n",
    "newFeatures = [f[1] for f in lexFields]\n",
    "\n",
    "for (lan, lexemes) in lexEntries.items():\n",
    "    for (lex, lexValues) in lexemes.items():\n",
    "        lexId = (lan, lex)\n",
    "        node = nodeFromLex.get(lexId, None)\n",
    "        if node is None:\n",
    "            continue\n",
    "        nodeFeatures.setdefault(\"lex\", {})[node] = lex\n",
    "        nodeFeatures.setdefault(\"languageISO\", {})[node] = langMap[lan]\n",
    "        nodeFeatures.setdefault(\"language\", {})[node] = langIMap[lan]\n",
    "        for (f, newF) in lexFields:\n",
    "            value = lexValues.get(f, None)\n",
    "            if value is not None:\n",
    "                nodeFeatures.setdefault(newF, {})[node] = value\n",
    "                if newF in extendFeatures:\n",
    "                    for wordNode in lexOccs[lexId]:\n",
    "                        nodeFeatures[newF][wordNode] = value\n",
    "        if DO_VOCALIZED_LEXEME:\n",
    "            for (f, vocValues) in vocFeatures.items():\n",
    "                value = vocValues.get((lan, lex), None)\n",
    "                if value is not None:\n",
    "                    nodeFeatures.setdefault(f, {})[node] = value\n",
    "                    for wordNode in lexOccs[lexId]:\n",
    "                        nodeFeatures[f][wordNode] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with various issues\n",
    "We address the issues listed under [various issues](#Various-issues) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".         47s Various tweaks in features                                                     .\n",
      "..............................................................................................\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, \"Various tweaks in features\")\n",
    "\n",
    "nodeFeatures[\"lex0\"] = {}\n",
    "nodeFeatures[\"lex_utf8\"] = {}\n",
    "nodeFeatures[\"languageISO\"] = {}\n",
    "\n",
    "geresh = chr(0x59C)\n",
    "\n",
    "contractFeatures = dict(lex_utf8=True, lex0=False, languageISO=False)\n",
    "# these are features that exists already on words, but are not covered by the lexicon\n",
    "# we have already test their consistency for lexeme nodes.\n",
    "# We copy their values\n",
    "# to the relevant lexeme nodes\n",
    "# lex_utf8 does already exist on word nodes, we can pick it up from F,\n",
    "# but lex0 must be picked up from nodeFeatures[\"lex0\"]\n",
    "\n",
    "for n in F.otype.s(\"word\"):\n",
    "    lex = F.lex.v(n)\n",
    "    lex_utf8 = F.lex_utf8.v(n)\n",
    "    if lex_utf8.endswith(geresh):\n",
    "        lex_utf8 = lex_utf8.rstrip(geresh)\n",
    "    lan = F.language.v(n)\n",
    "    nodeFeatures[\"lex0\"][n] = lex.rstrip(\"/[=\")\n",
    "    nodeFeatures[\"lex_utf8\"][n] = lex_utf8\n",
    "    nodeFeatures[\"languageISO\"][n] = langMap[lan]\n",
    "    nodeFeatures[\"language\"][n] = langIMap[lan]\n",
    "    \n",
    "for (lexId, lexNode) in nodeFromLex.items():\n",
    "    wordNodes = lexOccs[lexId]\n",
    "    wordNode = wordNodes[0]\n",
    "    for (ft, exists) in contractFeatures.items():\n",
    "        nodeFeatures[ft][lexNode] = Fs(ft).v(wordNode) if exists else nodeFeatures[ft][wordNode]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the `otype`, `otext` and `oslots` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      2m 44s Update the otype, oslots and otext features                                    .\n",
      "..............................................................................................\n"
     ]
    }
   ],
   "source": [
    "utils.caption(4, \"Update the otype, oslots and otext features\")\n",
    "edgeFeatures = {}\n",
    "\n",
    "metaData[\"otext\"] = dict()\n",
    "metaData[\"otext\"].update(T.config)\n",
    "metaData[\"otext\"].update(otextInfo)\n",
    "metaData[\"otype\"] = dict(valueType=\"str\")\n",
    "metaData[\"oslots\"] = dict(valueType=\"str\")\n",
    "\n",
    "for f in nodeFeatures:\n",
    "    if f in lexiconMeta:\n",
    "        metaData[f] = lexiconMeta[f]\n",
    "    elif f in coreMeta:\n",
    "        metaData[f] = coreMeta[f]\n",
    "    else:\n",
    "        metaData[f] = {}\n",
    "    metaData[f][\"valueType\"] = \"str\"\n",
    "    metaData[f][\"provenance\"] = \"from additional lexicon file provided by the ETCBC\"\n",
    "\n",
    "nodeFeatures[\"otype\"] = dict((n, F.otype.v(n)) for n in range(1, maxNode + 1))\n",
    "nodeFeatures[\"otype\"].update(otypeData)\n",
    "edgeFeatures[\"oslots\"] = dict(\n",
    "    (n, E.oslots.s(n)) for n in range(maxSlot + 1, maxNode + 1)\n",
    ")\n",
    "edgeFeatures[\"oslots\"].update(oslotsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      2m 45s Features that have new or modified data\n",
      "|      2m 45s \tgloss\n",
      "|      2m 45s \tlanguage\n",
      "|      2m 45s \tlanguageISO\n",
      "|      2m 45s \tlex\n",
      "|      2m 45s \tlex0\n",
      "|      2m 45s \tlex_utf8\n",
      "|      2m 45s \tls\n",
      "|      2m 45s \tnametype\n",
      "|      2m 45s \totype\n",
      "|      2m 45s \troot\n",
      "|      2m 45s \tsp\n",
      "|      2m 45s \tvoc_lex\n",
      "|      2m 45s \tvoc_lex_utf8\n",
      "|      2m 45s \toslots\n",
      "|      2m 45s Check voc_lex_utf8: בְּ רֵאשִׁית ברא אֱלֹהִים אֵת הַ שָׁמַיִם וְ אֶרֶץ\n"
     ]
    }
   ],
   "source": [
    "utils.caption(0, \"Features that have new or modified data\")\n",
    "for f in sorted(nodeFeatures) + sorted(edgeFeatures):\n",
    "    utils.caption(0, \"\\t{}\".format(f))\n",
    "\n",
    "if DO_VOCALIZED_LEXEME:\n",
    "    testNodes = range(maxNode + 1, maxNode + 10)\n",
    "    utils.caption(\n",
    "        0,\n",
    "        \"Check voc_lex_utf8: {}\".format(\n",
    "            \" \".join(nodeFeatures[\"voc_lex_utf8\"][n] for n in testNodes)\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the features to delete and list the new/changed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      2m 47s \tFeatures to remove\n",
      "|      2m 47s \tg_voc_lex\n",
      "|      2m 47s \tg_voc_lex_utf8\n"
     ]
    }
   ],
   "source": [
    "deleteFeatures = (\n",
    "    set(\n",
    "        \"\"\"\n",
    "    g_voc_lex\n",
    "    g_voc_lex_utf8\n",
    "\"\"\".strip().split()\n",
    "    )\n",
    "    if DO_VOCALIZED_LEXEME\n",
    "    else set()\n",
    ")\n",
    "\n",
    "if deleteFeatures:\n",
    "    utils.caption(0, \"\\tFeatures to remove\")\n",
    "    for f in sorted(deleteFeatures):\n",
    "        utils.caption(0, \"\\t{}\".format(f))\n",
    "else:\n",
    "    utils.caption(0, \"\\tNo features to remove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "changedDataFeatures = set(nodeFeatures) | set(edgeFeatures)\n",
    "changedFeatures = changedDataFeatures | {\"otext\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write new features\n",
    "Transform the collected information in feature-like data-structures, and write it all\n",
    "out to `.tf` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      2m 49s write new/changed features to TF ...                                           .\n",
      "..............................................................................................\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.caption(4, \"write new/changed features to TF ...\")\n",
    "TF = Fabric(locations=thisTempTf, silent=True)\n",
    "TF.save(nodeFeatures=nodeFeatures, edgeFeatures=edgeFeatures, metaData=metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffs\n",
    "\n",
    "Check differences with previous versions.\n",
    "\n",
    "The new dataset has been created in a temporary directory,\n",
    "and has not yet been copied to its destination.\n",
    "\n",
    "Here is your opportunity to compare the newly created features with the older features.\n",
    "You expect some differences in some features.\n",
    "\n",
    "We check the differences between the previous version of the features and what has been generated.\n",
    "We list features that will be added and deleted and changed.\n",
    "For each changed feature we show the first line where the new feature differs from the old one.\n",
    "We ignore changes in the metadata, because the timestamp in the metadata will always change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      3m 16s Check differences with previous version                                        .\n",
      "..............................................................................................\n",
      "|      3m 16s \t7 features to add\n",
      "|      3m 16s \t\tgloss\n",
      "|      3m 16s \t\tlanguageISO\n",
      "|      3m 16s \t\tlex0\n",
      "|      3m 16s \t\tnametype\n",
      "|      3m 16s \t\troot\n",
      "|      3m 16s \t\tvoc_lex\n",
      "|      3m 16s \t\tvoc_lex_utf8\n",
      "|      3m 16s \tno features to delete\n",
      "|      3m 16s \t8 features in common\n",
      "|      3m 16s language                  ... differences after the metadata\n",
      "|      3m 16s \tline 426592 OLD --><empty><--\n",
      "|      3m 16s \tline 426592 NEW -->1437602\tHebrew<--\n",
      "|      3m 16s \tline 426593 OLD --><empty><--\n",
      "|      3m 16s \tline 426593 NEW -->Hebrew<--\n",
      "|      3m 16s \tline 426594 OLD --><empty><--\n",
      "|      3m 16s \tline 426594 NEW -->Hebrew<--\n",
      "|      3m 16s \tline 426595 OLD --><empty><--\n",
      "|      3m 16s \tline 426595 NEW -->Hebrew<--\n",
      "\n",
      "|      3m 16s lex                       ... differences after the metadata\n",
      "|      3m 17s \tline 426592 OLD --><empty><--\n",
      "|      3m 17s \tline 426592 NEW -->1437602\tB<--\n",
      "|      3m 17s \tline 426593 OLD --><empty><--\n",
      "|      3m 17s \tline 426593 NEW -->R>CJT/<--\n",
      "|      3m 17s \tline 426594 OLD --><empty><--\n",
      "|      3m 17s \tline 426594 NEW -->BR>[<--\n",
      "|      3m 17s \tline 426595 OLD --><empty><--\n",
      "|      3m 17s \tline 426595 NEW -->>LHJM/<--\n",
      "\n",
      "|      3m 17s lex_utf8                  ... differences after the metadata\n",
      "|      3m 17s \tline      3 OLD -->ראשׁית֜<--\n",
      "|      3m 17s \tline      3 NEW -->ראשׁית<--\n",
      "|      3m 17s \tline      5 OLD -->אלהים֜<--\n",
      "|      3m 17s \tline      5 NEW -->אלהים<--\n",
      "|      3m 17s \tline      8 OLD -->שׁמים֜<--\n",
      "|      3m 17s \tline      8 NEW -->שׁמים<--\n",
      "|      3m 17s \tline     12 OLD -->ארץ֜<--\n",
      "|      3m 17s \tline     12 NEW -->ארץ<--\n",
      "\n",
      "|      3m 17s ls                        ... differences after the metadata\n",
      "|      3m 17s \tline 426592 OLD --><empty><--\n",
      "|      3m 17s \tline 426592 NEW -->1437611\tvbcp<--\n",
      "|      3m 17s \tline 426593 OLD --><empty><--\n",
      "|      3m 17s \tline 426593 NEW -->1437621\tquot<--\n",
      "|      3m 17s \tline 426594 OLD --><empty><--\n",
      "|      3m 17s \tline 426594 NEW -->1437627\tppre<--\n",
      "|      3m 17s \tline 426595 OLD --><empty><--\n",
      "|      3m 17s \tline 426595 NEW -->1437630\tpadv<--\n",
      "\n",
      "|      3m 17s oslots                    ... differences after the metadata\n",
      "|      3m 18s \tline 1011013 OLD --><empty><--\n",
      "|      3m 18s \tline 1011013 NEW -->1,84,197,220,241,270,318,330,334,428,435 ...<--\n",
      "|      3m 18s \tline 1011014 OLD --><empty><--\n",
      "|      3m 18s \tline 1011014 NEW -->2,4662,27812,41332,48285,53078,66102,796 ...<--\n",
      "|      3m 18s \tline 1011015 OLD --><empty><--\n",
      "|      3m 18s \tline 1011015 NEW -->3,381,535,545,550,724,736,2126,2137,2148 ...<--\n",
      "|      3m 18s \tline 1011016 OLD --><empty><--\n",
      "|      3m 18s \tline 1011016 NEW -->4,26,34,42,50,60,81,97,127,142,162,176,1 ...<--\n",
      "\n",
      "|      3m 18s otext                     ... differences\n",
      "|      3m 18s \tline      5 OLD -->@email=shebanq@ancient-data.org<--\n",
      "|      3m 18s \tline      5 NEW -->@dateWritten=2021-12-09T07:37:31Z<--\n",
      "|      3m 18s \tline      6 OLD -->@encoders=Constantijn Sikkel (QDF), Ulri ...<--\n",
      "|      3m 18s \tline      6 NEW -->@email=shebanq@ancient-data.org<--\n",
      "|      3m 18s \tline      7 OLD -->@fmt:lex-orig-full={g_lex_utf8} <--\n",
      "|      3m 18s \tline      7 NEW -->@encoders=Constantijn Sikkel (QDF), Ulri ...<--\n",
      "|      3m 18s \tline      8 OLD -->@fmt:lex-orig-plain={lex_utf8} <--\n",
      "|      3m 18s \tline      8 NEW -->@fmt:lex-orig-full={g_lex_utf8} <--\n",
      "\n",
      "|      3m 18s otype                     ... differences after the metadata\n",
      "|      3m 18s \tline     14 OLD --><empty><--\n",
      "|      3m 18s \tline     14 NEW -->1437602-1446831\tlex<--\n",
      "\n",
      "|      3m 18s sp                        ... differences after the metadata\n",
      "|      3m 18s \tline 426592 OLD --><empty><--\n",
      "|      3m 18s \tline 426592 NEW -->1437602\tprep<--\n",
      "|      3m 18s \tline 426593 OLD --><empty><--\n",
      "|      3m 18s \tline 426593 NEW -->subs<--\n",
      "|      3m 18s \tline 426594 OLD --><empty><--\n",
      "|      3m 18s \tline 426594 NEW -->verb<--\n",
      "|      3m 18s \tline 426595 OLD --><empty><--\n",
      "|      3m 18s \tline 426595 NEW -->subs<--\n",
      "\n",
      "|      3m 18s Done\n"
     ]
    }
   ],
   "source": [
    "utils.checkDiffs(thisTempTf, thisTf, only=changedFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliver\n",
    "\n",
    "Copy the new TF dataset from the temporary location where it has been created to its final destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      3m 20s Deliver features to /Users/werk/github/etcbc/bhsa/tf/2021                      .\n",
      "..............................................................................................\n",
      "|      3m 20s \tlex\n",
      "|      3m 20s \tgloss\n",
      "|      3m 20s \tnametype\n",
      "|      3m 20s \tvoc_lex_utf8\n",
      "|      3m 20s \toslots\n",
      "|      3m 20s \tlanguage\n",
      "|      3m 20s \tlanguageISO\n",
      "|      3m 20s \totype\n",
      "|      3m 20s \troot\n",
      "|      3m 21s \totext\n",
      "|      3m 21s \tsp\n",
      "|      3m 21s \tvoc_lex\n",
      "|      3m 21s \tls\n",
      "|      3m 21s \tlex_utf8\n",
      "|      3m 21s \tlex0\n",
      "..............................................................................................\n",
      ".      3m 21s Delete features from /Users/werk/github/etcbc/bhsa/tf/2021                     .\n",
      "..............................................................................................\n",
      "|      3m 21s \tg_voc_lex_utf8 ... deleted\n",
      "|      3m 21s \tg_voc_lex ... deleted\n"
     ]
    }
   ],
   "source": [
    "utils.deliverFeatures(\n",
    "    thisTempTf, thisTf, changedFeatures, deleteFeatures=deleteFeatures\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile TF\n",
    "\n",
    "We load the new features, use the new format, check some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................\n",
      ".      3m 26s Load and compile the new TF features                                           .\n",
      "..............................................................................................\n",
      "This is Text-Fabric 9.1.6\n",
      "Api reference : https://annotation.github.io/text-fabric/tf/cheatsheet.html\n",
      "\n",
      "106 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.49s T otype                from ~/github/etcbc/bhsa/tf/2021\n",
      "   |     9.77s T oslots               from ~/github/etcbc/bhsa/tf/2021\n",
      "   |     0.00s Dataset without structure sections in otext:no structure functions in the T-API\n",
      "   |     0.73s T lex0                 from ~/github/etcbc/bhsa/tf/2021\n",
      "   |     0.83s T lex_utf8             from ~/github/etcbc/bhsa/tf/2021\n",
      "   |      |     0.58s C __levels__           from otype, oslots, otext\n",
      "   |      |       14s C __order__            from otype, oslots, __levels__\n",
      "   |      |     0.62s C __rank__             from otype, __order__\n",
      "   |      |       17s C __levUp__            from otype, oslots, __rank__\n",
      "   |      |       11s C __levDown__          from otype, __levUp__, __rank__\n",
      "   |      |     4.69s C __boundary__         from otype, oslots, __rank__\n",
      "   |      |     0.06s C __sections__         from otype, oslots, otext, __levUp__, __levels__, book, chapter, verse\n",
      "   |     0.73s T gloss                from ~/github/etcbc/bhsa/tf/2021\n",
      "   |     0.69s T language             from ~/github/etcbc/bhsa/tf/2021\n",
      "   |     0.71s T languageISO          from ~/github/etcbc/bhsa/tf/2021\n",
      "   |     0.77s T lex                  from ~/github/etcbc/bhsa/tf/2021\n",
      "   |     0.69s T ls                   from ~/github/etcbc/bhsa/tf/2021\n",
      "   |     0.09s T nametype             from ~/github/etcbc/bhsa/tf/2021\n",
      "   |     0.17s T root                 from ~/github/etcbc/bhsa/tf/2021\n",
      "   |     0.72s T sp                   from ~/github/etcbc/bhsa/tf/2021\n",
      "   |     0.78s T voc_lex              from ~/github/etcbc/bhsa/tf/2021\n",
      "   |     0.86s T voc_lex_utf8         from ~/github/etcbc/bhsa/tf/2021\n",
      " 1m 06s All features loaded/computed - for details use TF.isLoaded()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Computed',\n",
       "  'computed-data',\n",
       "  ('C Computed', 'Call AllComputeds', 'Cs ComputedString')),\n",
       " ('Features', 'edge-features', ('E Edge', 'Eall AllEdges', 'Es EdgeString')),\n",
       " ('Fabric', 'loading', ('TF',)),\n",
       " ('Locality', 'locality', ('L Locality',)),\n",
       " ('Nodes', 'navigating-nodes', ('N Nodes',)),\n",
       " ('Features',\n",
       "  'node-features',\n",
       "  ('F Feature', 'Fall AllFeatures', 'Fs FeatureString')),\n",
       " ('Search', 'search', ('S Search',)),\n",
       " ('Text', 'text', ('T Text',))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.caption(4, \"Load and compile the new TF features\")\n",
    "\n",
    "TF = Fabric(locations=thisTf, modules=[\"\"])\n",
    "api = TF.load(\" \".join(changedDataFeatures))\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      4m 46s new format lex-trans-plain (using lex0): B R>CJT BR> >LHJM >T H CMJM W >T H >RY \n",
      "|      4m 46s lex_utf8 feature              : ב ראשׁית ברא אלהים את ה שׁמים ו את ה ארץ\n",
      "|      4m 46s language feature              : Hebrew Hebrew Hebrew Hebrew Hebrew Hebrew Hebrew Hebrew Hebrew Hebrew Hebrew\n",
      "..............................................................................................\n",
      ".      4m 46s Lexeme info for the first verse                                                .\n",
      "..............................................................................................\n",
      "|      4m 46s \tHebrew - B - 15542x\n",
      "|      4m 46s \t\tgloss           = in\n",
      "|      4m 46s \t\tls              = None\n",
      "|      4m 46s \t\tnametype        = None\n",
      "|      4m 46s \t\troot            = None\n",
      "|      4m 46s \t\tsp              = prep\n",
      "|      4m 46s \t\tvoc_lex         = B.:\n",
      "|      4m 46s \t\tvoc_lex_utf8    = בְּ\n",
      "|      4m 46s \tHebrew - R>CJT/ - 51x\n",
      "|      4m 46s \t\tgloss           = beginning\n",
      "|      4m 46s \t\tls              = None\n",
      "|      4m 46s \t\tnametype        = None\n",
      "|      4m 46s \t\troot            = R>C\n",
      "|      4m 46s \t\tsp              = subs\n",
      "|      4m 46s \t\tvoc_lex         = R;>CIJT\n",
      "|      4m 46s \t\tvoc_lex_utf8    = רֵאשִׁית\n",
      "|      4m 46s \tHebrew - BR>[ - 48x\n",
      "|      4m 46s \t\tgloss           = create\n",
      "|      4m 46s \t\tls              = None\n",
      "|      4m 46s \t\tnametype        = None\n",
      "|      4m 46s \t\troot            = None\n",
      "|      4m 46s \t\tsp              = verb\n",
      "|      4m 46s \t\tvoc_lex         = BR>\n",
      "|      4m 46s \t\tvoc_lex_utf8    = ברא\n",
      "|      4m 46s \tHebrew - >LHJM/ - 2601x\n",
      "|      4m 46s \t\tgloss           = god(s)\n",
      "|      4m 46s \t\tls              = None\n",
      "|      4m 46s \t\tnametype        = None\n",
      "|      4m 46s \t\troot            = None\n",
      "|      4m 46s \t\tsp              = subs\n",
      "|      4m 46s \t\tvoc_lex         = >:ELOHIJM\n",
      "|      4m 46s \t\tvoc_lex_utf8    = אֱלֹהִים\n",
      "|      4m 46s \tHebrew - >T - 10987x\n",
      "|      4m 46s \t\tgloss           = <object marker>\n",
      "|      4m 46s \t\tls              = None\n",
      "|      4m 46s \t\tnametype        = None\n",
      "|      4m 46s \t\troot            = None\n",
      "|      4m 46s \t\tsp              = prep\n",
      "|      4m 46s \t\tvoc_lex         = >;T\n",
      "|      4m 46s \t\tvoc_lex_utf8    = אֵת\n",
      "|      4m 46s \tHebrew - H - 30386x\n",
      "|      4m 46s \t\tgloss           = the\n",
      "|      4m 46s \t\tls              = None\n",
      "|      4m 46s \t\tnametype        = None\n",
      "|      4m 46s \t\troot            = None\n",
      "|      4m 46s \t\tsp              = art\n",
      "|      4m 46s \t\tvoc_lex         = HA\n",
      "|      4m 46s \t\tvoc_lex_utf8    = הַ\n",
      "|      4m 46s \tHebrew - CMJM/ - 421x\n",
      "|      4m 46s \t\tgloss           = heavens\n",
      "|      4m 46s \t\tls              = None\n",
      "|      4m 46s \t\tnametype        = None\n",
      "|      4m 46s \t\troot            = CMH\n",
      "|      4m 46s \t\tsp              = subs\n",
      "|      4m 46s \t\tvoc_lex         = C@MAJIM\n",
      "|      4m 46s \t\tvoc_lex_utf8    = שָׁמַיִם\n",
      "|      4m 46s \tHebrew - W - 50272x\n",
      "|      4m 46s \t\tgloss           = and\n",
      "|      4m 46s \t\tls              = None\n",
      "|      4m 46s \t\tnametype        = None\n",
      "|      4m 46s \t\troot            = None\n",
      "|      4m 46s \t\tsp              = conj\n",
      "|      4m 46s \t\tvoc_lex         = W:\n",
      "|      4m 46s \t\tvoc_lex_utf8    = וְ\n",
      "|      4m 46s \tHebrew - >T - 10987x\n",
      "|      4m 46s \t\tgloss           = <object marker>\n",
      "|      4m 46s \t\tls              = None\n",
      "|      4m 46s \t\tnametype        = None\n",
      "|      4m 46s \t\troot            = None\n",
      "|      4m 46s \t\tsp              = prep\n",
      "|      4m 46s \t\tvoc_lex         = >;T\n",
      "|      4m 46s \t\tvoc_lex_utf8    = אֵת\n",
      "|      4m 46s \tHebrew - H - 30386x\n",
      "|      4m 46s \t\tgloss           = the\n",
      "|      4m 46s \t\tls              = None\n",
      "|      4m 46s \t\tnametype        = None\n",
      "|      4m 46s \t\troot            = None\n",
      "|      4m 46s \t\tsp              = art\n",
      "|      4m 46s \t\tvoc_lex         = HA\n",
      "|      4m 46s \t\tvoc_lex_utf8    = הַ\n",
      "|      4m 46s \tHebrew - >RY/ - 2504x\n",
      "|      4m 46s \t\tgloss           = earth\n",
      "|      4m 46s \t\tls              = None\n",
      "|      4m 46s \t\tnametype        = None\n",
      "|      4m 46s \t\troot            = None\n",
      "|      4m 46s \t\tsp              = subs\n",
      "|      4m 46s \t\tvoc_lex         = >EREY\n",
      "|      4m 46s \t\tvoc_lex_utf8    = אֶרֶץ\n"
     ]
    }
   ],
   "source": [
    "features = [f[1] for f in lexFields] + (\n",
    "    [\"voc_lex\", \"voc_lex_utf8\"] if DO_VOCALIZED_LEXEME else []\n",
    ")\n",
    "\n",
    "\n",
    "def showLex(w):\n",
    "    info = dict((f, Fs(f).v(w)) for f in features)\n",
    "    utils.caption(\n",
    "        0,\n",
    "        \"\\t{} - {} - {}x\".format(\n",
    "            F.language.v(w),\n",
    "            F.lex.v(w),\n",
    "            len(L.d(w, otype=\"word\")),\n",
    "        ),\n",
    "    )\n",
    "    for f in sorted(info):\n",
    "        utils.caption(0, \"\\t\\t{:<15} = {}\".format(f, info[f]))\n",
    "\n",
    "\n",
    "fmt = \"lex-trans-plain\"\n",
    "utils.caption(\n",
    "    0,\n",
    "    \"{:<30}: {}\".format(\n",
    "        \"new format {} (using lex0)\".format(fmt),\n",
    "        T.text(range(1, 12), fmt=fmt),\n",
    "    ),\n",
    ")\n",
    "utils.caption(\n",
    "    0,\n",
    "    \"{:<30}: {}\".format(\n",
    "        \"lex_utf8 feature\",\n",
    "        \" \".join(F.lex_utf8.v(w) for w in range(1, 12)),\n",
    "    ),\n",
    ")\n",
    "utils.caption(\n",
    "    0,\n",
    "    \"{:<30}: {}\".format(\n",
    "        \"language feature\",\n",
    "        \" \".join(F.language.v(w) for w in range(1, 12)),\n",
    "    ),\n",
    ")\n",
    "utils.caption(4, \"Lexeme info for the first verse\")\n",
    "\n",
    "for w in range(1, 12):\n",
    "    showLex(L.u(w, otype=\"lex\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCRIPT:\n",
    "    stop(good=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
